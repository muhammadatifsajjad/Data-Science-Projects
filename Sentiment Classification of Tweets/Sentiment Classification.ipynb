{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Author: Muhammad Atif\n",
    "\n",
    "Python version: 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will use the tweets in the <i>twitter_samples</i> corpus included with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length, in characters, of the tweets in the corpus is 103.85176666666666\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk \n",
    "import numpy as np\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score    \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# store tweets in a list as strings \n",
    "allTweets = twitter_samples.strings()\n",
    "\n",
    "# iterate over the tweets and count the number of characters across all the tweets\n",
    "charCount = 0\n",
    "for tweet in allTweets:\n",
    "    charCount += len(tweet)\n",
    "\n",
    "# calculate and print average length, in characters, of the tweets in the corpus    \n",
    "avgLenOfCharsPerTweet = charCount / len(allTweets)\n",
    "print('Average length, in characters, of the tweets in the corpus is', avgLenOfCharsPerTweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtags (i.e. topic tags which start with #) pose an interesting tokenisation problem because they often include multiple words written without spaces or capitalization. We will use a regular expression to extract all hashtags of length 8 or longer which consist only of lower case letters (other than the # at the beginning which is stripped off as part of the extraction process) without tokenising the entire tweet as part of this process. The hashtag might occur at the beginning or the end of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of hashtags across all the tweets in the corpus is 1411\n"
     ]
    }
   ],
   "source": [
    "# regular expression to find hashtags\n",
    "regex = r'(?<=^#)[a-z]{8,}$|(?<=^#)[a-z]{8,}(?=\\s)|(?<=\\s#)[a-z]{8,}$|(?<=\\s#)[a-z]{8,}(?=\\s)'\n",
    "\n",
    "# extract all hashtags using the above regex\n",
    "listOfHashtags = [hashtag for hashtag in [re.findall(regex, tweet) for tweet in allTweets] if hashtag != []]\n",
    "\n",
    "# count and print total number of hashtags collected\n",
    "hashtagsCount = 0\n",
    "for hashtags in listOfHashtags:\n",
    "    hashtagsCount += len(hashtags)\n",
    "    \n",
    "print('Total number of hashtags across all the tweets in the corpus is', hashtagsCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will tokenise the hashtags. To do this, we will implement a reversed version of the MaxMatch algorithm, where matching begins at the end of the hashtag and progresses backwards. NLTK has a list of words that can be used for matching while being careful about efficiency with respect to doing word lookups. One extra challenge is that the provided list of words includes only lemmas: MaxMatch algorithm should match inflected forms by converting them into lemmas before matching. Note that the list of words is incomplete, and, if we are unable to make any longer match, we will default to matching a single letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 20 hashtags:\n",
      " [['time', 'show'], ['c', 'b', 'b', 'biased'], ['band', 'li', 'mi'], ['night', 'scot'], ['democrats'], ['worrying'], ['labour', 'falling'], ['debate', 'leaders'], ['campaign', 'wow', 'security', 'social', 'lies', 'tory'], ['election'], ['c', 'b', 'b', 'biased'], ['doorstep', 'labour'], ['c', 'b', 'b', 'biased', 'con', 'blab', 'li'], ['debate', 'c', 'b', 'b', 'fandom', 'li', 'mi'], ['parliament', 'k', 'u'], ['tax', 'bedroom', 'disability'], ['is', 'nab', 'can', 'green', 'vote'], ['stings', 'u', 'h', 'li', 'el', 'lan', 'l'], ['tax', 'bedroom', 'disability'], ['bankrupt']]\n"
     ]
    }
   ],
   "source": [
    "# code for lemmatization\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "    \n",
    "# words is a Python list\n",
    "wordsCorpus = nltk.corpus.words.words()\n",
    "\n",
    "# reverse MaxMatch algorithm\n",
    "def reverseMaxMatch(word):\n",
    "    remWordsList = \"\"\n",
    "    n = len(word)\n",
    "\n",
    "    for i in range(n):\n",
    "        if lemmatize(word[i:]) in wordsCorpus:      # if the lemmatized word is found in corpus, add it to the list\n",
    "            tokenizedHashtags.append(word[i:])      \n",
    "            if(remWordsList != \"\"):                 # recursive call on words that have not yet been compared\n",
    "                reverseMaxMatch(remWordsList)       \n",
    "                break\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            remWordsList += word[i]                 # add left out alphabet to remaining words list to be compared in the recursive call\n",
    "\n",
    "# initialize final list of tokenized hashtags            \n",
    "listOfTokenizedHashtags = []\n",
    "\n",
    "# call reverse MaxMatch algorithm hashtags to get tokenized hashtags \n",
    "for hashtags in listOfHashtags:\n",
    "    tokenizedHashtags = []  \n",
    "    for hashtag in hashtags:         \n",
    "        reverseMaxMatch(hashtag)\n",
    "    listOfTokenizedHashtags.append(tokenizedHashtags)\n",
    "\n",
    "# print out the last 20 tokenized hashtags\n",
    "print('Last 20 hashtags:\\n', listOfTokenizedHashtags[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The twitter_sample corpus has two subcorpora corresponding to positive and negative tweets. We will iterate through these two corpora and build training, development, and test sets for use with Scikit-learn. We will exclude stopwords (from the built-in NLTK list) and tokens with non-alphabetic characters (this is very important because emoticons were used to build the corpus, if we don't remove them performance will be artificially high). We will randomly split each subcorpus, using 80% of the tweets for training, 10% for development, and 10% for testing; ensuring to do this <b>before</b> combining the tweets from the positive/negative subcorpora, so that the sets are <i>stratified</i>, i.e. the exact ratio of positive and negative tweets is preserved across the three sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tokenized positive and negative tweets from twitter_samples\n",
    "posTweets = nltk.corpus.twitter_samples.tokenized(\"positive_tweets.json\")\n",
    "negTweets = nltk.corpus.twitter_samples.tokenized(\"negative_tweets.json\")\n",
    "\n",
    "# randomly split each subcorpus in 80:10:10 ratio for training:development:testing\n",
    "posTweetsTrain, posTweetsDev = train_test_split(posTweets, test_size=0.2)\n",
    "posTweetsDev, posTweetsTest = train_test_split(posTweetsDev, test_size=0.5)\n",
    "\n",
    "negTweetsTrain, negTweetsDev = train_test_split(negTweets, test_size=0.2)\n",
    "negTweetsDev, negTweetsTest = train_test_split(negTweetsDev, test_size=0.5)\n",
    "\n",
    "# combine positive and negative tweets data\n",
    "# positive tweets are labelled as 1, and negative tweets are labelled as 0\n",
    "xTrain = list(posTweetsTrain)\n",
    "xTrain.extend(negTweetsTrain)\n",
    "\n",
    "yTrain = list(np.repeat(1, 4000))\n",
    "yTrain.extend(list(np.repeat(0, 4000)))\n",
    "\n",
    "xDev = list(posTweetsDev)\n",
    "xDev.extend(negTweetsDev)\n",
    "\n",
    "yDev = list(np.repeat(1, 500))\n",
    "yDev.extend(list(np.repeat(0, 500)))\n",
    "\n",
    "xTest = list(posTweetsTest)\n",
    "xTest.extend(negTweetsTest)\n",
    "\n",
    "yTest = list(np.repeat(1, 500))\n",
    "yTest.extend(list(np.repeat(0, 500)))\n",
    "\n",
    "# remove unnecessary variables from memory\n",
    "del posTweets, negTweets, negTweetsTrain, negTweetsDev, negTweetsTest, posTweetsTrain, posTweetsDev, posTweetsTest\n",
    "\n",
    "# get bag of words after doing lower-casing, removing stopwords and non-alphabetic characters\n",
    "stopwords = set(stopwords.words('english'))\n",
    "def getBOWLoweredNoStopwords(tweet):\n",
    "    BOW = {}\n",
    "    for word in tweet:\n",
    "        word = word.lower()\n",
    "        regex = r'^[a-z]+$'         #Remove non-alphabetic tokens\n",
    "        word = str(re.findall(regex, word))\n",
    "        if word not in stopwords and word != '[]' :\n",
    "            BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "def prepareTweetsData(allTweets):\n",
    "    feature_matrix = []\n",
    "    for tweet in allTweets:\n",
    "        feature_dict = getBOWLoweredNoStopwords(tweet) \n",
    "        feature_matrix.append(feature_dict)\n",
    "    return feature_matrix\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "trainDataset = vectorizer.fit_transform(prepareTweetsData(xTrain))\n",
    "devDataset = vectorizer.transform(prepareTweetsData(xDev))\n",
    "testDataset = vectorizer.transform(prepareTweetsData(xTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build some classifiers. Here, we'll be comparing Naive Bayes and Logistic Regression. For each, we need to first find a good value for their main regularisation (hyper)parameters. We will use the development set we created for this tuning process; without using cross-validation in the training set, or involving the test set in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Logistic Regression Classifier Accuracy with parameters, penalty=l1, C=1 :  0.771\n",
      "Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=1 :  0.768\n",
      "Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=0.9 :  0.767\n",
      "Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=0.8 :  0.768\n",
      "Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=0.7 :  0.767\n",
      "Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=0.6 :  0.765\n",
      "Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=0.5 :  0.765\n",
      "Naive Bayes Classifier Accuracy with parameter, alpha=1.5 :  0.757\n",
      "Naive Bayes Classifier Accuracy with parameter, alpha=1.6 :  0.757\n",
      "Naive Bayes Classifier Accuracy with parameter, alpha=1.7 :  0.755\n",
      "Naive Bayes Classifier Accuracy with parameter, alpha=1.8 :  0.755\n",
      "Naive Bayes Classifier Accuracy with parameter, alpha=1.9 :  0.754\n",
      "Naive Bayes Classifier Accuracy with parameter, alpha=2 :  0.757\n",
      "Naive Bayes Classifier Accuracy with parameter, alpha=2.1 :  0.758\n"
     ]
    }
   ],
   "source": [
    "# create logistic regression classifier\n",
    "def logisticRegressionModel(penaltyVal, cVal, predDataset):\n",
    "    logReg = LogisticRegression(penalty = penaltyVal, C = cVal) # initialize logistic regression model with parameter values\n",
    "    logReg.fit(trainDataset, yTrain)    # train the logistic regression model using training set\n",
    "    return logReg.predict(predDataset)   # predict using development dataset\n",
    "\n",
    "# create multinomial naive bayes classifier\n",
    "def naiveBayesModel(alphaVal, predDataset):\n",
    "    naiveBayes = MultinomialNB(alpha = alphaVal) # initialize logistic regression model with parameter values\n",
    "    naiveBayes.fit(trainDataset, yTrain) # train the naive bayes model using training set\n",
    "    return naiveBayes.predict(predDataset) # predict using development dataset\n",
    "\n",
    "#Hyperparameter tuning for logistic regression   \n",
    "devPredLogReg = logisticRegressionModel('l1', 1, devDataset)\n",
    "print('\\n\\nLogistic Regression Classifier Accuracy with parameters, penalty=l1, C=1 : ', accuracy_score(yDev, devPredLogReg))\n",
    "\n",
    "devPredLogReg = logisticRegressionModel('l2', 1, devDataset)\n",
    "print('Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=1 : ', accuracy_score(yDev, devPredLogReg))\n",
    "\n",
    "devPredLogReg = logisticRegressionModel('l2', 0.9, devDataset)\n",
    "print('Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=0.9 : ', accuracy_score(yDev, devPredLogReg))\n",
    "\n",
    "devPredLogReg = logisticRegressionModel('l2', 0.8, devDataset)\n",
    "print('Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=0.8 : ', accuracy_score(yDev, devPredLogReg))\n",
    "\n",
    "devPredLogReg = logisticRegressionModel('l2', 0.7, devDataset)\n",
    "print('Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=0.7 : ', accuracy_score(yDev, devPredLogReg))\n",
    "\n",
    "devPredLogReg = logisticRegressionModel('l2', 0.6, devDataset)\n",
    "print('Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=0.6 : ', accuracy_score(yDev, devPredLogReg))\n",
    "\n",
    "devPredLogReg = logisticRegressionModel('l2', 0.5, devDataset)\n",
    "print('Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=0.5 : ', accuracy_score(yDev, devPredLogReg))\n",
    "\n",
    "#Hyperparameter tuning for Naive Bayes  \n",
    "devPredNaiveBayes = naiveBayesModel(1.5, devDataset)\n",
    "print('Naive Bayes Classifier Accuracy with parameter, alpha=1.5 : ', accuracy_score(yDev, devPredNaiveBayes))\n",
    " \n",
    "devPredNaiveBayes = naiveBayesModel(1.6, devDataset)\n",
    "print('Naive Bayes Classifier Accuracy with parameter, alpha=1.6 : ', accuracy_score(yDev, devPredNaiveBayes))\n",
    " \n",
    "devPredNaiveBayes = naiveBayesModel(1.7, devDataset)\n",
    "print('Naive Bayes Classifier Accuracy with parameter, alpha=1.7 : ', accuracy_score(yDev, devPredNaiveBayes))\n",
    "\n",
    "devPredNaiveBayes = naiveBayesModel(1.8, devDataset)\n",
    "print('Naive Bayes Classifier Accuracy with parameter, alpha=1.8 : ', accuracy_score(yDev, devPredNaiveBayes))\n",
    "  \n",
    "devPredNaiveBayes = naiveBayesModel(1.9, devDataset)\n",
    "print('Naive Bayes Classifier Accuracy with parameter, alpha=1.9 : ', accuracy_score(yDev, devPredNaiveBayes))\n",
    "\n",
    "devPredNaiveBayes = naiveBayesModel(2, devDataset)\n",
    "print('Naive Bayes Classifier Accuracy with parameter, alpha=2 : ', accuracy_score(yDev, devPredNaiveBayes))\n",
    "  \n",
    "devPredNaiveBayes = naiveBayesModel(2.1, devDataset)\n",
    "print('Naive Bayes Classifier Accuracy with parameter, alpha=2.1 : ', accuracy_score(yDev, devPredNaiveBayes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best settings found, let's compare the two classifiers based on performance in the test set in terms of accuracy and macroaveraged f-score for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=0.6 : Accuracy -  0.764  f1-score -  0.757700205338809\n",
      "Naive Bayes Classifier Accuracy and f1-score with parameter, alpha=1.6 : Accuracy -  0.766  f1-score -  0.7552301255230126\n"
     ]
    }
   ],
   "source": [
    "# predict using test dataset\n",
    "testPredLogReg = logisticRegressionModel('l2', 0.6, testDataset)\n",
    "print('Logistic Regression Classifier Accuracy with parameters, penalty=l2, C=0.6 : Accuracy - ', accuracy_score(yTest, testPredLogReg), ' f1-score - ', f1_score(yTest, testPredLogReg))\n",
    "\n",
    "testPredNaiveBayes = naiveBayesModel(1.6, testDataset)\n",
    "print('Naive Bayes Classifier Accuracy and f1-score with parameter, alpha=1.6 : Accuracy - ', accuracy_score(yTest, testPredNaiveBayes), ' f1-score - ', f1_score(yTest, testPredNaiveBayes))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
