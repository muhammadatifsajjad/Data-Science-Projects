{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Author: Muhammad Atif\n",
    "\n",
    "Python version: 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Overview</b>: In this notebook, we will be adapting a POS tagger to Twitter data, starting from a tagger trained on Penn Treebank, using prior information on the Twitter tagset to obtain better performance. We will also analyse your results in a more fine-grained way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our first task is to preprocess the data. We will use two datasets for training: 1) the Penn Treebank sample and 2) the Twitter samples data. In order to adapt the tagger to the Twitter data we need to built a *joint* vocabulary containing all the word types in PTB and the twitter_samples corpora.\n",
    "\n",
    "The vocabulary and the tagset will be stored in Python dictionaries, mapping each word (or tag) to an index (integer).\n",
    "\n",
    "Let's start with the PTB data. We will iterate over all sentences and words, and build the vocabulary and the tagset, ensuring to <b>lowercase</b> words before they are added to the dictionary. We will also generate the preprocessed corpus. It should be a list where each element is a tagged sentence, represented as another list of (word, tag) indices (which should correspond to the original words/tags).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First sentence in preprocessed PTB train corpus: \n",
      " [(0, 0), (1, 0), (2, 1), (3, 2), (4, 3), (5, 4), (2, 1), (6, 5), (7, 6), (8, 7), (9, 8), (10, 9), (11, 7), (12, 4), (13, 8), (14, 0), (15, 2), (16, 10)]\n",
      "\n",
      "Index for the word electricity: \n",
      " 1095\n",
      "\n",
      "Length of the full tagset: \n",
      " 46\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import urllib\n",
    "from nltk.corpus import treebank, twitter_samples\n",
    "\n",
    "corpus = treebank.tagged_sents()\n",
    "vocab = {}\n",
    "tagset = {}\n",
    "\n",
    "preProcessedCorpusPTBTrain = []\n",
    "for sent in corpus:\n",
    "    num_sent = []\n",
    "    for word, tag in sent:\n",
    "        wi = vocab.setdefault(word.lower().strip(), len(vocab))\n",
    "        ti = tagset.setdefault(tag, len(tagset))\n",
    "        num_sent.append((wi, ti))\n",
    "    preProcessedCorpusPTBTrain.append(num_sent)\n",
    "    \n",
    "print('\\nFirst sentence in preprocessed PTB train corpus: \\n', preProcessedCorpusPTBTrain[0])\n",
    "print('\\nIndex for the word electricity: \\n', vocab['electricity'])\n",
    "print('\\nLength of the full tagset: \\n', len(tagset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do the same with the twitter_samples dataset. From now on, we will refer this dataset as the **training** tweets. Since this data is not tagged, the preprocessed corpus will be a list where each element is another list containing indices only (instead of (word, tag) tuples). Besides generating the corpus, we will also **update** the vocabulary with the new words from this corpus.\n",
    "\n",
    "There are two things to keep in mind when doing this process:\n",
    "\n",
    "1) We will perform a bit more of preprocessing in this dataset, besides lowercasing. Specifically, we will replace special tokens with special symbols, as follows:\n",
    "- Username mentions are tokens that start with '@': replace these tokens with 'USER_TOKEN'\n",
    "- Hashtags are tokens that start with '#': replace these with 'HASHTAG_TOKEN'\n",
    "- Retweets are represented as the token 'RT' (or 'rt' if you lowercase first): replace these with 'RETWEET_TOKEN'\n",
    "- URLs are tokens that start with 'https://' or 'http://': replace these with 'URL_TOKEN'\n",
    "\n",
    "2) **We will not create a new vocabulary**. Instead, we will update the vocabulary built from PTB with any new words present in this corpus. These should *include* the special tokens defined above but *not* the original un-preprocessed tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First sentence in preprocessed twitter_samples train corpus: \n",
      " [11387, 182, 11388, 11389]\n",
      "\n",
      "Index for the word electricity: \n",
      " 1095\n",
      "\n",
      "Index for HASHTAG_TOKEN: \n",
      " 11409\n"
     ]
    }
   ],
   "source": [
    "preProcessedCorpusTwitterTrain = []\n",
    "for tokens in twitter_samples.tokenized():\n",
    "    num_sent = []\n",
    "    for word in tokens:\n",
    "        #pre-processing on words\n",
    "        word = word.lower().strip()\n",
    "        word = re.sub(r'^@.*', 'USER_TOKEN', word)\n",
    "        word = re.sub(r'^#.*', 'HASHTAG_TOKEN', word)\n",
    "        word = re.sub(r'^http(s)?://.*', 'URL_TOKEN', word)\n",
    "        word = re.sub('^rt$', 'RETWEET_TOKEN', word)\n",
    "\n",
    "        wi = vocab.setdefault(word, len(vocab))\n",
    "        num_sent.append(wi)   \n",
    "    preProcessedCorpusTwitterTrain.append(num_sent)\n",
    "\n",
    "print('\\nFirst sentence in preprocessed twitter_samples train corpus: \\n', preProcessedCorpusTwitterTrain[0])\n",
    "print('\\nIndex for the word electricity: \\n', vocab['electricity'])\n",
    "print('\\nIndex for HASHTAG_TOKEN: \\n', vocab['HASHTAG_TOKEN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will preprocess the tagged twitter corpus used in W7 (Ritter et al.). This dataset will be referred from now on as **test** tweets. Before we do that, we will update the tagset.\n",
    "\n",
    "This dataset has a few extra tags, besides the PTB ones. These were added to incorporate specific phenomena that happens on Twitter:\n",
    "- \"USR\": username mentions\n",
    "- \"HT\": hashtags\n",
    "- \"RT\": retweets\n",
    "- \"URL\": URL addresses\n",
    "\n",
    "Notice that these special tags correspond to the special tokens we preprocessed before. These steps will be important in Part 3 later.\n",
    "\n",
    "There a few additional tags which are not specific to Twitter but are not present in the PTB sample:\n",
    "- \"VPP\"\n",
    "- \"TD\"\n",
    "- \"O\"\n",
    "\n",
    "We will add these new seven tags to the tagset we built when reading the PTB corpus.\n",
    "\n",
    "Another task is to add an extra type to the vocabulary: `<unk>`. This is in order to account for unknown or out-of-vocabulary words.\n",
    "\n",
    "Finally, we will build two \"inverted indices\" for the vocabulary and the tagset. These should be lists, where the \"i\"-th element should contain the word (or tag) corresponding to the index \"i\" in the vocabulary (or tagset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index for <unk>: \n",
      " 26069\n",
      "\n",
      "Length of resulting tagset: \n",
      " 53\n"
     ]
    }
   ],
   "source": [
    "tagset.setdefault('USR', len(tagset))\n",
    "tagset.setdefault('HT', len(tagset))\n",
    "tagset.setdefault('RT', len(tagset))\n",
    "tagset.setdefault('URL', len(tagset))\n",
    "tagset.setdefault('VPP', len(tagset))\n",
    "tagset.setdefault('TD', len(tagset))\n",
    "tagset.setdefault('O', len(tagset))\n",
    "\n",
    "vocab.setdefault('<unk>', len(vocab))\n",
    "\n",
    "invVocab = [None] * len(vocab)\n",
    "for word, index in vocab.items():\n",
    "    invVocab[index] = word\n",
    "invTagset = [None] * len(tagset)\n",
    "for tag, index in tagset.items():\n",
    "    invTagset[index] = tag\n",
    "    \n",
    "print('\\nIndex for ''<unk>'': \\n', vocab['<unk>'])\n",
    "print('\\nLength of resulting tagset: \\n', len(invTagset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read the test tweets storing them in the same format as the PTB corpora (list of lists containing (word, tag) index tuples). We will do the same preprocessing steps that we did for the training tweets (lowercasing + replace special tokens). However, **we will not** update the vocabulary. Why? Because the test set should simulate a real-world scenario, where out-of-vocabulary words can appear. Instead, after preprocessing each word, we will check if that word is in the vocabulary. If yes, just replace it with its index, otherwise we will replace it with the index for the `<unk>` token.\n",
    "\n",
    "When reading the POS tags for the test tweets we will do some additional preprocessing. There are three tags in this dataset which correspond to PTB tags but are represented with different names:\n",
    "- \"(\". In PTB, this is represented as \"-LRB-\"\n",
    "- \")\". In PTB, this is represented as \"-RRB-\"\n",
    "- \"NONE\". In PTB, this is represented as \"-NONE-\"\n",
    "\n",
    "As we build the corpus for the test tweets, we will check if the tag for a word is one of the above. If yes, we will use the PTB equivalent instead. In practice, it is sufficient to ensure that we use the correct index for the corresponding tag, using our tagset dictionary. This concept is sometimes referred as *tag harmonisation*, where two different tagsets are mapped to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First sentence in preprocessed twitter test corpus: \n",
      " [(11392, 46), (61, 19), (114, 11), (8, 7), (3224, 8), (170, 9), (325, 33), (1325, 19), (2375, 22), (3205, 12), (182, 9), (799, 2), (1522, 3), (16, 10), (8490, 0), (1146, 0), (2495, 0), (14039, 43), (26069, 0), (16, 10), (4263, 17), (1760, 4), (9464, 8), (2259, 17), (888, 4), (741, 8), (16, 10)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    urllib.request.urlretrieve(\"https://github.com/aritter/twitter_nlp/raw/master/data/annotated/pos.txt\",\"pos.txt\")\n",
    "except: # Python 2\n",
    "    urllib.urlretrieve(\"https://github.com/aritter/twitter_nlp/raw/master/data/annotated/pos.txt\",\"pos.txt\")\n",
    "    \n",
    "preProcessedCorpusTwitterTest = []\n",
    "with open('pos.txt') as f:\n",
    "    wordsTags = []\n",
    "    for line in f:\n",
    "        if line.strip() == '':\n",
    "            preProcessedCorpusTwitterTest.append(wordsTags)\n",
    "            wordsTags = []\n",
    "        else:\n",
    "            word, tag = line.strip().split()          \n",
    "            #pre-processing on words\n",
    "            word = word.lower().strip()\n",
    "            word = re.sub(r'^@.*', 'USER_TOKEN', word)\n",
    "            word = re.sub(r'^#.*', 'HASHTAG_TOKEN', word)\n",
    "            word = re.sub(r'^http(s)?://.*', 'URL_TOKEN', word)\n",
    "            word = re.sub('^rt$', 'RETWEET_TOKEN', word)\n",
    "            #pre-processing on tags\n",
    "            tag = tag.replace(\"(\", \"-LRB-\")\n",
    "            tag = tag.replace(\")\", \"-RRB-\")\n",
    "            tag = tag.replace(\"NONE\", \"-NONE-\")\n",
    "         \n",
    "            wi = vocab.get(word, vocab.get('<unk>'))\n",
    "            ti = tagset.get(tag)\n",
    "            wordsTags.append((wi, ti))\n",
    "           \n",
    "print('\\nFirst sentence in preprocessed twitter test corpus: \\n', preProcessedCorpusTwitterTest[0])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Running the PTB tagger on the test tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next task is to train a POS tagger on the PTB data and try it on the test tweets. \n",
    "\n",
    "Our first task is to encapsulate the HMM training code into a function. We will name our function `count`. This function will take these input parameters:\n",
    "- A tagged corpus, in the format described above (list of lists containing (word, tag) index tuples).\n",
    "- The vocabulary (a dict).\n",
    "- The tagset (a dict).\n",
    "\n",
    "Output return values will contain:\n",
    "- The initial tag probabilities (a vector).\n",
    "- The transition probabilities (a matrix).\n",
    "- The emission probabilities (a matrix).\n",
    "\n",
    "Notice that we pass vocabulary and tagset explicitly as parameters. This is to ensure our tagger can take into account the words in the training tweets and the extra tags. We will initialise the probabilities with an `eps` value, to ensure we end up with non-zero probabilities for unseen events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(corpus, vocab, tagset):\n",
    "    S = len(tagset)\n",
    "    V = len(vocab)\n",
    "    \n",
    "    # initalise\n",
    "    eps = 0.1\n",
    "    pi = eps * np.ones(S)\n",
    "    A = eps * np.ones((S, S))\n",
    "    O = eps * np.ones((S, V))\n",
    "    \n",
    "    # count\n",
    "    for sent in corpus:\n",
    "        last_tag = None\n",
    "        for word, tag in sent:\n",
    "            O[tag, word] += 1\n",
    "            if last_tag == None:\n",
    "                pi[tag] += 1\n",
    "            else:\n",
    "                A[last_tag, tag] += 1\n",
    "            last_tag = tag\n",
    "            \n",
    "    # normalise\n",
    "    pi /= np.sum(pi)\n",
    "    for s in range(S):\n",
    "        O[s,:] /= np.sum(O[s,:])\n",
    "        A[s,:] /= np.sum(A[s,:])\n",
    "    \n",
    "    return pi, A, O\n",
    "    \n",
    "[initialMatrix, transitionMatrix, emissionMatrix] = count(preProcessedCorpusPTBTrain, vocab, tagset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a function for Viterbi. The input parameters are:\n",
    "- The parameters (probabilities) of your HMM (a tuple (initial, transition, emission)).\n",
    "- The input words (a list with numbers).\n",
    "\n",
    "The output is a list of (word, tag) indices, containing the original input word and the predicted tag.\n",
    "\n",
    "We will run Viterbi on the test tweets and store the predictions in a list (might take a few seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First sentence of predicted list: \n",
      " [(11392, 27), (61, 19), (114, 11), (8, 7), (3224, 8), (170, 9), (325, 33), (1325, 19), (2375, 22), (3205, 12), (182, 9), (799, 2), (1522, 3), (16, 10), (8490, 29), (1146, 8), (2495, 8), (14039, 10), (26069, 38), (16, 10), (4263, 29), (1760, 4), (9464, 8), (2259, 17), (888, 4), (741, 8), (16, 10)]\n"
     ]
    }
   ],
   "source": [
    "def viterbi(params, observations):\n",
    "    pi, A, O = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[:,:] = float('-inf')\n",
    "    backpointers = np.zeros((M, S), 'int')\n",
    "    \n",
    "    # base case\n",
    "    alpha[0, :] = pi * O[:,observations[0]]\n",
    "    \n",
    "    # recursive case\n",
    "    for t in range(1, M):\n",
    "        for s2 in range(S):\n",
    "            for s1 in range(S):\n",
    "                score = alpha[t-1, s1] * A[s1, s2] * O[s2, observations[t]]\n",
    "                if score > alpha[t, s2]:\n",
    "                    alpha[t, s2] = score\n",
    "                    backpointers[t, s2] = s1\n",
    "    \n",
    "    # now follow backpointers to resolve the state sequence\n",
    "    ss = []\n",
    "    ss.append(np.argmax(alpha[M-1,:]))\n",
    "    for i in range(M-1, 0, -1):\n",
    "        ss.append(backpointers[i, ss[-1]])\n",
    "        \n",
    "    return list(zip(observations, list(reversed(ss))))\n",
    "\n",
    "predictions = []\n",
    "for sent in preProcessedCorpusTwitterTest:\n",
    "    encoded_sent = [wordTags[0] for wordTags in sent]\n",
    "    pred = viterbi((initialMatrix, transitionMatrix, emissionMatrix), encoded_sent)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "print('\\nFirst sentence of predicted list: \\n', predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now evaluate the results. We will write a function that takes (word, tag) lists as inputs and outputs the tag sequence using the original tags in the tagset. Our inputs will be a sentence and the tag inverted index you built before.\n",
    "\n",
    "We will run this function on the predictions we obtained above **and** the test tweets, storing them in two separate lists. Finally, we will flat our predictions into a single list and do the same for the test tweets and report accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\n",
      " 63.7 %\n"
     ]
    }
   ],
   "source": [
    "def getOrigTags(sentences, tagInvIndex):\n",
    "    tagSequence = []\n",
    "    for wordTagsList in sentences:\n",
    "        for wordTags in wordTagsList:\n",
    "            tagSequence.append(tagInvIndex[wordTags[1]])\n",
    "    return tagSequence\n",
    "\n",
    "tagSequenceTest = getOrigTags(preProcessedCorpusTwitterTest, invTagset)\n",
    "tagSequencePred = getOrigTags(predictions, invTagset)\n",
    "\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "print('\\nAccuracy:\\n', round(acc(tagSequenceTest, tagSequencePred) * 100, 1), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Adapting the tagger using prior information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our task is to adapt the tagger using prior information. What do we mean by that? Remember from part 1 that the twitter tagset has some extra tags, related to special tokens such as mentions and hashtags. In other words, **we know beforehand** that these special tokens **should** have these tags. However, because these tags never appear in the PTB data, the tagger has no such information. We are going to add this in order to improve the tagger.\n",
    "\n",
    "To recap, we know these things about the twitter data:\n",
    "- username mentions should have the tag 'USR'\n",
    "- hashtags should have the tag 'HT'\n",
    "- retweet tokens should have the tag 'RT'\n",
    "- URL tokens should have the tag 'URL'\n",
    "\n",
    "Remember how we replace these tokens with unique special ones (such as 'USER_TOKEN')? Our task now is to adapt the emission probabilities for these tokens. We will modify the emission matrix: assign 1.0 probability for the emission P('USER_TOKEN'|'USR') and 0.0 for P(word|'USR') for all other words, doing the same for the other three special tags.\n",
    "\n",
    "In order to do that, we will use the vocabulary and tagset dictionaries in order to obtain the indices for the corresponding words and tags. Then, we will use the indices to find the values in the emission matrix and modify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emission Matrix:\n",
      " [[9.15369893e-05 1.74752434e-04 8.32154448e-06 ... 8.32154448e-06\n",
      "  8.32154448e-06 8.32154448e-06]\n",
      " [1.33457894e-05 1.33457894e-05 6.51955158e-01 ... 1.33457894e-05\n",
      "  1.33457894e-05 1.33457894e-05]\n",
      " [1.62522347e-05 1.62522347e-05 1.62522347e-05 ... 1.62522347e-05\n",
      "  1.62522347e-05 1.62522347e-05]\n",
      " ...\n",
      " [3.83582662e-05 3.83582662e-05 3.83582662e-05 ... 3.83582662e-05\n",
      "  3.83582662e-05 3.83582662e-05]\n",
      " [3.83582662e-05 3.83582662e-05 3.83582662e-05 ... 3.83582662e-05\n",
      "  3.83582662e-05 3.83582662e-05]\n",
      " [3.83582662e-05 3.83582662e-05 3.83582662e-05 ... 3.83582662e-05\n",
      "  3.83582662e-05 3.83582662e-05]]\n"
     ]
    }
   ],
   "source": [
    "emissionMatrix[tagset['USR']] = 0.0\n",
    "emissionMatrix[tagset['USR']][vocab['USER_TOKEN']] = 1.0\n",
    "\n",
    "emissionMatrix[tagset['HT']] = 0.0\n",
    "emissionMatrix[tagset['HT']][vocab['HASHTAG_TOKEN']] = 1.0\n",
    "\n",
    "emissionMatrix[tagset['URL']] = 0.0\n",
    "emissionMatrix[tagset['URL']][vocab['URL_TOKEN']] = 1.0\n",
    "\n",
    "emissionMatrix[tagset['RT']] = 0.0\n",
    "emissionMatrix[tagset['RT']][vocab['RETWEET_TOKEN']] = 1.0\n",
    "\n",
    "print('\\nEmission Matrix:\\n', emissionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate our new tagger on the test tweets again. We will report accuracy but also do a fine-grained error analysis. We will print the F-scores for **each tag**, reporting the tags that performed the best and the worse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\n",
      " 69.5 %\n",
      "\n",
      "Clasification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          #       0.00      0.00      0.00         0\n",
      "          $       0.00      0.00      0.00         0\n",
      "         ''       0.03      0.20      0.06        91\n",
      "          ,       0.85      1.00      0.92       303\n",
      "      -LRB-       0.00      0.00      0.00        32\n",
      "     -NONE-       0.00      0.00      0.00         2\n",
      "      -RRB-       0.04      0.15      0.07        34\n",
      "          .       0.72      0.83      0.77       875\n",
      "          :       0.97      0.76      0.85       562\n",
      "         CC       0.96      0.88      0.92       305\n",
      "         CD       0.59      0.59      0.59       268\n",
      "         DT       0.74      0.93      0.82       825\n",
      "         EX       0.38      0.80      0.52        10\n",
      "         FW       0.00      0.00      0.00         3\n",
      "         HT       0.98      0.98      0.98       135\n",
      "         IN       0.81      0.88      0.85      1091\n",
      "         JJ       0.64      0.59      0.61       670\n",
      "        JJR       0.48      0.74      0.58        31\n",
      "        JJS       0.84      0.81      0.82        26\n",
      "         LS       0.00      0.00      0.00         1\n",
      "         MD       0.53      0.97      0.69       181\n",
      "         NN       0.79      0.63      0.70      1931\n",
      "        NNP       0.60      0.27      0.37      1159\n",
      "       NNPS       0.00      0.00      0.00         8\n",
      "        NNS       0.43      0.54      0.48       393\n",
      "          O       0.00      0.00      0.00         1\n",
      "        PDT       0.00      0.00      0.00         1\n",
      "        POS       0.39      0.78      0.52        36\n",
      "        PRP       0.86      0.82      0.84      1106\n",
      "       PRP$       0.84      0.86      0.85       234\n",
      "         RB       0.71      0.76      0.73       680\n",
      "        RBR       0.62      0.25      0.36        20\n",
      "        RBS       0.08      0.33      0.12         3\n",
      "         RP       0.64      0.45      0.53       110\n",
      "         RT       1.00      1.00      1.00       152\n",
      "        SYM       0.00      0.00      0.00        13\n",
      "         TD       0.00      0.00      0.00         1\n",
      "         TO       0.84      0.96      0.90       264\n",
      "         UH       1.00      0.00      0.00       493\n",
      "        URL       0.99      0.90      0.95       183\n",
      "        USR       0.98      0.96      0.97       464\n",
      "         VB       0.65      0.70      0.68       660\n",
      "        VBD       0.77      0.74      0.75       306\n",
      "        VBG       0.88      0.50      0.64       303\n",
      "        VBN       0.43      0.63      0.51       140\n",
      "        VBP       0.78      0.64      0.70       527\n",
      "        VBZ       0.69      0.78      0.73       342\n",
      "        VPP       0.00      0.00      0.00         1\n",
      "        WDT       0.36      0.47      0.41        19\n",
      "         WP       0.97      0.74      0.84        47\n",
      "        WP$       0.00      0.00      0.00         0\n",
      "        WRB       1.00      0.81      0.90       143\n",
      "         ``       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.76      0.70      0.70     15185\n",
      "\n",
      "\n",
      "Best tags having F1-Score >= 0.9:\n",
      "\"TO\", \"WRB\", \",\", \"CC\", \"URL\", \"USR\", \"HT\", \"RT\"\n",
      "\n",
      "Worst tags having F1-Score <= 0.1:\n",
      "\"#\", \"$\", \"-LRB-\", \"-NONE-\", \"FW\", \"LS\", \"NNPS\", \"O\", \"PDT\", \"SYM\", \"TD\", \"UH\", \"VPP\", \"WP$\", \"``\", \"''\", \"-RRB-\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for sent in preProcessedCorpusTwitterTest:\n",
    "    encoded_sent = [wordTags[0] for wordTags in sent]\n",
    "    pred = viterbi((initialMatrix, transitionMatrix, emissionMatrix), encoded_sent)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "tagSequenceTest = getOrigTags(preProcessedCorpusTwitterTest, invTagset)\n",
    "tagSequencePred = getOrigTags(predictions, invTagset)\n",
    "\n",
    "print('\\nAccuracy:\\n', round(acc(tagSequenceTest, tagSequencePred) * 100, 1), '%')    \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClasification report:\\n', classification_report(tagSequenceTest, tagSequencePred))\n",
    "print(\"\\nBest tags having F1-Score >= 0.9:\\n\\\"TO\\\", \\\"WRB\\\", \\\",\\\", \\\"CC\\\", \\\"URL\\\", \\\"USR\\\", \\\"HT\\\", \\\"RT\\\"\") \n",
    "print(\"\\nWorst tags having F1-Score <= 0.1:\\n\\\"#\\\", \\\"$\\\", \\\"-LRB-\\\", \\\"-NONE-\\\", \\\"FW\\\", \\\"LS\\\", \\\"NNPS\\\", \\\"O\\\", \\\"PDT\\\", \\\"SYM\\\", \\\"TD\\\", \\\"UH\\\", \\\"VPP\\\", \\\"WP$\\\", \\\"``\\\", \\\"''\\\", \\\"-RRB-\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, based on the information we got above, we will do some analysis. Why did the tagger performed worse on the tags we mentioned above? How can we improve the tagger? We will inspect some instances manually to write our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training is done on a subset of Penn Treebank (PTB) corpus which is freely available with NLTK. As such, there are a lot of words which are not present in the PTB corpus we used - word vocabulary from PTB corpus consists of 11387 word types which increases to 26069 words when we add new words from twitter training dataset (which does not have a corresponding POS tag). Also, there are tags that either do not appear in PTB dataset at all e.g. \"TD\", \"VPP\", \"O\", or they appear with very low frequency e.g. \"UH\" which appears only 3 times in PTB. Further, all the worse performing tags have a very low support meaning that they occurr very infrequently in the test dataset. One exception, however, is the \"UH\" (interjection) tag which appears 493 times in test dataset, and the reason for it's poor performance is because twitter dataset is expected to have smileys and internet language slangs such as 'lol', 'hahaha', ':)', 'omg' etc. having \"UH\" tags which are not present in PTB tagged corpus (PTB only has three words 'OK', 'no', 'Oh' having \"UH\" tags).\n",
    "\n",
    "Some ways to improve the tagger are (1) to include smileys and common internet language expression slangs, such as the ones mentioned above, with \"UH\" tag in our dictionaries and explicitly set the emission probabilities just like we did for special tokens like \"URL_TOKEN\", (2) to use a more comprehensive tagged corpus, and (3) instead of bigrams, use model based on trigrams i.e. compute probability of a tag given its last two tags.</b>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
