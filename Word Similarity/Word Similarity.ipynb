{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Student Name: Muhammad Atif\n",
    "\n",
    "Python version: 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([('professor', 'doctor'), ('stock', 'egg'), ('baby', 'mother'), ('car', 'automobile'), ('journey', 'voyage'), ('coast', 'shore'), ('brother', 'monk'), ('journey', 'car'), ('coast', 'hill'), ('monk', 'slave'), ('coast', 'forest'), ('psychology', 'doctor'), ('psychology', 'mind'), ('psychology', 'health'), ('psychology', 'science'), ('computer', 'laboratory'), ('canyon', 'landscape'), ('century', 'year'), ('doctor', 'personnel'), ('school', 'center'), ('word', 'similarity'), ('hotel', 'reservation'), ('type', 'kind'), ('equipment', 'maker'), ('luxury', 'car'), ('soap', 'opera'), ('planet', 'people')])\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import operator\n",
    "import math\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "# Load 'combined.tab' file in dictionary\n",
    "with open('C:\\\\Users\\\\ma_at\\\\Desktop\\\\Web Search and Text Analysis - COMP90042\\\\Assignments\\\\Assignment 2\\\\combined.tab') as tabFile:\n",
    "    next(tabFile)\n",
    "    tabSepWords = (line.split('\\t') for line in tabFile)\n",
    "    wordSimDict = {(words[0],words[1]):float(words[2]) for words in tabSepWords}  \n",
    "    \n",
    "# for each paragraph in brown corpus, store a list of lower-cased, lemmatized word types\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "brownParas = []\n",
    "for paragraphs in brown.paras():\n",
    "    wordTypes = set()\n",
    "    wordTypes.update([lemmatizer.lemmatize(words.lower()) for sentences in paragraphs for words in sentences])\n",
    "    brownParas.append(wordTypes)\n",
    "\n",
    "# create a dictionary of document frequency for word types in brown corpus\n",
    "wordTypeDocFreqDict = {}\n",
    "for paragraphs in brownParas:\n",
    "    for word in paragraphs:\n",
    "        wordTypeDocFreqDict[word] = wordTypeDocFreqDict.get(word,0) + 1\n",
    "\n",
    "# filter word pairs where frequency of either one of them is less than 10\n",
    "for word1, word2 in list(wordSimDict):\n",
    "    if (wordTypeDocFreqDict.get(word1,0) < 10) | (wordTypeDocFreqDict.get(word2,0) < 10):\n",
    "        wordSimDict.pop((word1, word2), None)\n",
    "\n",
    "# store single noun primary sense of words in dictionary, and\n",
    "# filter word pairs where single noun primary sense is not found\n",
    "primarySenseSynsetDict = {}\n",
    "for wordPairs in list(wordSimDict):\n",
    "    for word in wordPairs:\n",
    "        haveNounPrimarySense = False\n",
    "        synsets = wn.synsets(word)\n",
    "        if (len(synsets) == 1 and synsets[0].pos() == 'n'):\n",
    "            haveNounPrimarySense = True\n",
    "            primarySenseSynsetDict[word] = synsets[0]\n",
    "        elif (len(synsets) > 1):\n",
    "            lemmaCounts = {}\n",
    "            for synset in synsets:\n",
    "                for lemma in synset.lemmas():\n",
    "                    lemmaName = lemma.name()\n",
    "                    if lemmaName == word:\n",
    "                        lemmaCounts[lemma, synset] = lemma.count()  \n",
    "            lemmaCounts = sorted(lemmaCounts.items(), key=operator.itemgetter(1), reverse=True)            \n",
    "            if (len(lemmaCounts) == 1) and (lemmaCounts[0][0][1].pos() == 'n') and (lemmaCounts[0][1] >= 5):\n",
    "                haveNounPrimarySense = True\n",
    "            elif len(lemmaCounts) > 1:\n",
    "                if(lemmaCounts[0][1] >= 5) and (lemmaCounts[0][1] >= lemmaCounts[1][1]*5) and (lemmaCounts[0][0][1].pos() == 'n'):\n",
    "                    haveNounPrimarySense = True\n",
    "            if haveNounPrimarySense == True:   \n",
    "                primarySenseSynsetDict[word] = lemmaCounts[0][0][1]\n",
    "        if haveNounPrimarySense == False:\n",
    "            wordSimDict.pop(wordPairs, None) \n",
    "            break\n",
    "\n",
    "print(wordSimDict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create several dictionaries with similarity scores for pairs of words in our test set. The first of these is the Wu-Palmer scores derived from the hypernym relationships in WordNet, which we will calculate using the primary sense for each word derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('professor', 'doctor'): 0.5, ('stock', 'egg'): 0.11764705882352941, ('baby', 'mother'): 0.5, ('car', 'automobile'): 1.0, ('journey', 'voyage'): 0.8571428571428571, ('coast', 'shore'): 0.9090909090909091, ('brother', 'monk'): 0.5714285714285714, ('journey', 'car'): 0.09523809523809523, ('coast', 'hill'): 0.6666666666666666, ('monk', 'slave'): 0.6666666666666666, ('coast', 'forest'): 0.16666666666666666, ('psychology', 'doctor'): 0.1111111111111111, ('psychology', 'mind'): 0.5714285714285714, ('psychology', 'health'): 0.21052631578947367, ('psychology', 'science'): 0.9411764705882353, ('computer', 'laboratory'): 0.35294117647058826, ('canyon', 'landscape'): 0.3333333333333333, ('century', 'year'): 0.8333333333333334, ('doctor', 'personnel'): 0.13333333333333333, ('school', 'center'): 0.13333333333333333, ('word', 'similarity'): 0.3333333333333333, ('hotel', 'reservation'): 0.375, ('type', 'kind'): 0.9473684210526315, ('equipment', 'maker'): 0.5, ('luxury', 'car'): 0.1111111111111111, ('soap', 'opera'): 0.2222222222222222, ('planet', 'people'): 0.18181818181818182}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary of wordpair/Wu-Palmer-similarity mappings for filtered word pairs\n",
    "wuPalmerSimilarityDict={}\n",
    "for word1, word2 in wordSimDict:\n",
    "    wuPalmerSimilarityDict[word1,word2] = primarySenseSynsetDict[word1].wup_similarity(primarySenseSynsetDict[word2])\n",
    "\n",
    "print(wuPalmerSimilarityDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will calculate Positive PMI (PPMI) for our word pairs using statistics derived from the Brown using the same set up as we did to calculate document frequency above: paragraphs as documents, lemmatized, lower-cased, and with term frequency information removed by conversion to Python sets. We will use the basic method for calculating PPMI which is appropriate for any possible definition of co-occurrence (here, appearing in the same paragraph), but we will only calculate PPMI for the words in our test set. We will avoid building the entire co-occurrence matrix, instead we will keep track of the sums for the probabilities as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('professor', 'doctor'): 0.0, ('stock', 'egg'): 1.8174736272140593, ('baby', 'mother'): 3.1068514542000756, ('car', 'automobile'): 3.284928059255019, ('journey', 'voyage'): 0.0, ('coast', 'shore'): 4.630747773460183, ('brother', 'monk'): 2.8992677183777067, ('journey', 'car'): 0.0, ('coast', 'hill'): 1.2130606957673897, ('monk', 'slave'): 0.0, ('coast', 'forest'): 3.0505076829814297, ('psychology', 'doctor'): 3.5625762708186035, ('psychology', 'mind'): 2.7796743924855387, ('psychology', 'health'): 0.0, ('psychology', 'science'): 5.078497127110109, ('computer', 'laboratory'): 0.0, ('canyon', 'landscape'): 0.0, ('century', 'year'): 0.85521193298008, ('doctor', 'personnel'): 2.2186218696012423, ('school', 'center'): 0.744045575429721, ('word', 'similarity'): 0.0, ('hotel', 'reservation'): 2.891047211572738, ('type', 'kind'): 0.6500752376975433, ('equipment', 'maker'): 4.283313403192924, ('luxury', 'car'): 2.272328022475385, ('soap', 'opera'): 4.221195813265069, ('planet', 'people'): 0.4092477799069862}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary of wordpair/PPMI-similarity mappings for filtered word pairs\n",
    "pmiSimilarityDict={}\n",
    "totalParasCount = float(len(brownParas))\n",
    "for word1, word2 in wordSimDict:\n",
    "    wordCount1 = 0\n",
    "    wordCount2 = 0\n",
    "    bothWordCount = 0\n",
    "    for paras in brownParas:\n",
    "        if word1 in paras:\n",
    "            wordCount1 += 1\n",
    "            if word2 in paras:\n",
    "                bothWordCount += 1\n",
    "        if word2 in paras:\n",
    "            wordCount2 += 1\n",
    "    probCalc = (bothWordCount/totalParasCount)/((wordCount1/totalParasCount)*(wordCount2/totalParasCount))\n",
    "    pmiSimilarityDict[word1, word2] = 0.0 if probCalc==0 else math.log(probCalc, 2)\n",
    "\n",
    "print(pmiSimilarityDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will derive similarity scores using the LSA method, i.e. apply SVD and truncate to get a dense vector and then use cosine similarity between the two vectors for each word pair. We will be constructing a matrix where the (non-sparse) rows correspond to words in the vocabulary, and the (sparse) columns correspond to the texts where they appear. Again, we will use the Brown corpus, in the same format as with PMI and document frequency. After we have a matrix in the correct format, we will use truncatedSVD in Sci-kit learn to produce dense vectors of length 500, and then use cosine similarity to produce similarities for our word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('professor', 'doctor'): 0.1282666695038288, ('stock', 'egg'): 0.14018079535015326, ('baby', 'mother'): 0.33791842030742014, ('car', 'automobile'): 0.33016543021476574, ('journey', 'voyage'): 0.11860079401341417, ('coast', 'shore'): 0.40707475890629174, ('brother', 'monk'): 0.11141221551167657, ('journey', 'car'): -0.017884878217367062, ('coast', 'hill'): 0.1665997471401639, ('monk', 'slave'): -0.049648664986159385, ('coast', 'forest'): 0.1051220289732262, ('psychology', 'doctor'): 0.1220134726939699, ('psychology', 'mind'): 0.11359551564838555, ('psychology', 'health'): 0.014612476126077745, ('psychology', 'science'): 0.259081350444586, ('computer', 'laboratory'): 0.14034774071438472, ('canyon', 'landscape'): 0.10381126884703273, ('century', 'year'): 0.06959542605469127, ('doctor', 'personnel'): 0.06324958418247362, ('school', 'center'): 0.04368174614910669, ('word', 'similarity'): 0.004895650526370865, ('hotel', 'reservation'): 0.06454546381060666, ('type', 'kind'): 0.025631411554342898, ('equipment', 'maker'): 0.26718871571584724, ('luxury', 'car'): 0.07562939325934293, ('soap', 'opera'): -0.00563359285269982, ('planet', 'people'): 0.033211724022358835}\n"
     ]
    }
   ],
   "source": [
    "# bag-of-words implementation\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word.lower()] = BOW.get(word.lower(),0) + 1\n",
    "    return BOW\n",
    "    \n",
    "# get frequency of words in paras using word types list created in Q1. \n",
    "# matrix of 0's and 1's depending on if the word exists in a paragraph or not\n",
    "texts = []\n",
    "for paras in brownParas:\n",
    "    texts.append(get_BOW(paras))\n",
    "\n",
    "# create words-paragraph frequency matrix\n",
    "vectorizer = DictVectorizer()\n",
    "brownMatrix = vectorizer.fit_transform(texts).transpose()\n",
    "\n",
    "# get dense vectors of length 500 using truncated SVD\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "brownMatrixSVD = svd.fit_transform(brownMatrix)\n",
    "\n",
    "# create dictionary of wordpair/cosine-similarity mappings using LSA method for filtered word pairs\n",
    "cosineSimilarityDict = {}\n",
    "for word1, word2 in wordSimDict:\n",
    "    word1Index = vectorizer.feature_names_.index(word1)\n",
    "    word2Index = vectorizer.feature_names_.index(word2)\n",
    "    cosSim = 1 - cos_distance(brownMatrixSVD[word1Index,:], brownMatrixSVD[word2Index,:])\n",
    "    cosineSimilarityDict[word1, word2] = cosSim\n",
    "\n",
    "print(cosineSimilarityDict)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will derive a similarity score from word2vec vectors, using the Gensim interface. Check the Gensim word2vec tutorial for details on the API: https://radimrehurek.com/gensim/models/word2vec.html. Again, we will use the Brown for this, but for word2vec we don't need to worry about paragraphs and will train our model at the sentence level instead. Our vectors should have the same number of dimensions as LSA (500), and we will run for 50 iterations. This may take a while (several minutes). We will extract the similarites directly from the Gensim model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('professor', 'doctor'): 0.1029803229917908, ('stock', 'egg'): 0.1535156438485933, ('baby', 'mother'): 0.23415810120914327, ('car', 'automobile'): 0.17324409909345384, ('journey', 'voyage'): 0.4791619430328846, ('coast', 'shore'): 0.40639991421327903, ('brother', 'monk'): 0.04261563182237669, ('journey', 'car'): 0.19540108613671048, ('coast', 'hill'): 0.44374781062422103, ('monk', 'slave'): 0.009620027853437595, ('coast', 'forest'): 0.2879224341437675, ('psychology', 'doctor'): -0.027752614111583283, ('psychology', 'mind'): 0.05155980765556281, ('psychology', 'health'): 0.16711893567800892, ('psychology', 'science'): 0.31320292307617803, ('computer', 'laboratory'): 0.19272994938677682, ('canyon', 'landscape'): 0.16166988678899866, ('century', 'year'): 0.30780071132661774, ('doctor', 'personnel'): -0.05511129237329291, ('school', 'center'): -0.03064820064330967, ('word', 'similarity'): 0.039037391290843, ('hotel', 'reservation'): 0.05672515585654653, ('type', 'kind'): 0.2617343193270218, ('equipment', 'maker'): 0.18189017556402345, ('luxury', 'car'): 0.14847815012824178, ('soap', 'opera'): -0.02474933914939556, ('planet', 'people'): -0.011206916852775858}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary of wordpair/word2vec-similarity mappings for filtered word pairs using sentences from brown corpus\n",
    "brownSentences = nltk.corpus.brown.sents()\n",
    "model = Word2Vec(brownSentences, min_count=5, size=500, iter=50)\n",
    "word2vecSimilarityDict = {}\n",
    "for word1, word2 in wordSimDict:\n",
    "    word2vecSimilarityDict[word1, word2] = model.wv.similarity(word1, word2)\n",
    "\n",
    "print(word2vecSimilarityDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will compare all the similarities we've created to the gold standard we loaded and filtered in the first step. For this, we will use the Pearson correlation co-efficient (`pearsonr`), which is included in scipy (`scipy.stats`). The data for the two datasets needs to be in the same order for correct comparison using correlation. We will write a general function and then apply it to each of the similarity score dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient compared with Gold Standard:\n",
      "------------------------------------------------------------\n",
      "Cosine:  0.36048303868834\n",
      "PMI:  0.21406331437316534\n",
      "Wu-Palmer:  0.45669274063664006\n",
      "Word2Vec:  0.3151240968418435\n"
     ]
    }
   ],
   "source": [
    "# compare similarities with the gold standard using pearson correlation co-efficient\n",
    "wordSimGoldStanardList = list(wordSimDict.values())\n",
    "def pearsonCorrelation(wordSimilarityDict):\n",
    "    wordSimilarityList = list(wordSimilarityDict.values())\n",
    "    return pearsonr(wordSimGoldStanardList, wordSimilarityList)[0]\n",
    "\n",
    "print('Pearson correlation coefficient compared with Gold Standard:')\n",
    "print('------------------------------------------------------------')\n",
    "print('Cosine: ', pearsonCorrelation(cosineSimilarityDict))\n",
    "print('PMI: ', pearsonCorrelation(pmiSimilarityDict))\n",
    "print('Wu-Palmer: ', pearsonCorrelation(wuPalmerSimilarityDict))\n",
    "print('Word2Vec: ', pearsonCorrelation(word2vecSimilarityDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A final word\n",
    "\n",
    "Normally, we would not use a corpus as small as the Brown for the purposes of building distributional word vectors. Also, note that filtering our test set to just words we are likely to do well on would typically be considered cheating."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
